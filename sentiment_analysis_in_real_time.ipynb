{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d4d2c7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Forbes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691c738",
   "metadata": {},
   "source": [
    "#### WHAT IS KAFKA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cc78d",
   "metadata": {},
   "source": [
    "Event streaming is the practice of capturing data in real-time from event sources like databases, sensors,\n",
    "mobile devices, cloud services, and software applications in the form of streams of events; storing these\n",
    "event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in\n",
    "real-time as well as retrospectively; and routing the event streams to different destination technologies\n",
    "as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right\n",
    "information is at the right place, at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0c0da",
   "metadata": {},
   "source": [
    "#### EVENTS\n",
    "\n",
    "An event records the fact that “something happened” in the world or in your business. It is also called record\n",
    "or message in the documentation. When you read or write data to Kafka, you do this in the form of events.\n",
    "Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here’s an example\n",
    "event:\n",
    "\n",
    "- Event key : \" Alice \"\n",
    "- Event value : \" Made a payment of $200 to Bob \"\n",
    "- Event timestamp : \" Jun . 25 , 2020 at 2:06 p . m .\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726aa38",
   "metadata": {},
   "source": [
    "#### PRODUCERS / CONSUMERS\n",
    "\n",
    "Producers are those client applications that publish (write) events to Kafka, and consumers are those that\n",
    "subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and\n",
    "agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for.\n",
    "For example, producers never need to wait for consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bc3dd",
   "metadata": {},
   "source": [
    "#### TOPICS\n",
    "\n",
    "Events are organized and durably stored in topics. Topics in Kafka are always multi-producer and multisubscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many\n",
    "consumers that subscribe to these events. Topics in Kafka are always multi-producer and multi-subscriber: a\n",
    "topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers\n",
    "that subscribe to these events. Events in a topic can be read as often as needed, you define for how long\n",
    "Kafka should retain your events through a per-topic configuration setting, after which old events will be\n",
    "discarded.\n",
    "\n",
    "![Illustrations](image_kafka2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c33d7a",
   "metadata": {},
   "source": [
    "#### PROJECT SUMMARY\n",
    "\n",
    "The goal of this project is to create an end-to-end Machine Learning project, including :\n",
    "- extract tweets of specifics topics from Twitter, in real-time using Apache Kafka\n",
    "- transform, using you trained-model for sentiments analysis classification\n",
    "- load data into a data-warehouse using PostgreSQL\n",
    "- real-time dashboard, to monitor the results for each topics using PowerBI \n",
    "Each parts can be start independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abc282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "time.sleep(5) # Pause de 5 secondes après le chargement de la page\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from confluent_kafka import Producer, Consumer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from io import StringIO\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec87ad6",
   "metadata": {},
   "source": [
    "#### EXTRACT\n",
    "\n",
    "First you need to install Kafka (and Zookeeper to manage it) on your system. At least 4GB of RAM is needed.\n",
    "You can use this tutorial : https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafkaon-ubuntu-18-04\n",
    "You will use the website forbes.com and try to make your code generic in a way that it caters to multiple themes and publish them into the Kafka server using topics. You can have multiple #keyword to monitor for one specific show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b995ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x0000000102a0ae18 chromedriver + 4627992\n1   chromedriver                        0x0000000102a02b43 chromedriver + 4594499\n2   chromedriver                        0x0000000102600e4a chromedriver + 392778\n3   chromedriver                        0x000000010264c41d chromedriver + 701469\n4   chromedriver                        0x000000010264c5b1 chromedriver + 701873\n5   chromedriver                        0x0000000102690214 chromedriver + 979476\n6   chromedriver                        0x000000010266e89d chromedriver + 841885\n7   chromedriver                        0x000000010268d6d9 chromedriver + 968409\n8   chromedriver                        0x000000010266e613 chromedriver + 841235\n9   chromedriver                        0x000000010263f3da chromedriver + 648154\n10  chromedriver                        0x000000010263fd1e chromedriver + 650526\n11  chromedriver                        0x00000001029ca890 chromedriver + 4364432\n12  chromedriver                        0x00000001029cfc41 chromedriver + 4385857\n13  chromedriver                        0x00000001029afb2e chromedriver + 4254510\n14  chromedriver                        0x00000001029d0969 chromedriver + 4389225\n15  chromedriver                        0x00000001029a1e69 chromedriver + 4197993\n16  chromedriver                        0x00000001029f1b78 chromedriver + 4524920\n17  chromedriver                        0x00000001029f1d57 chromedriver + 4525399\n18  chromedriver                        0x0000000102a02783 chromedriver + 4593539\n19  libsystem_pthread.dylib             0x00007ff8061141d3 _pthread_start + 125\n20  libsystem_pthread.dylib             0x00007ff80610fbd3 thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Appeler la fonction avec l'URL openAI\u001b[39;00m\n\u001b[1;32m     60\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinance\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 61\u001b[0m df \u001b[38;5;241m=\u001b[39m get_data(query)\n\u001b[1;32m     62\u001b[0m df\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      3\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.forbes.com/search/?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# temps pour que e boutton \"More articles\" soit cliquable\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m more_articles_button \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[1;32m      8\u001b[0m     EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch-more\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Cliquer sur le bouton \"More articles\" jusqu'à ce qu'il n'y en ait plus\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m more_articles_button:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x0000000102a0ae18 chromedriver + 4627992\n1   chromedriver                        0x0000000102a02b43 chromedriver + 4594499\n2   chromedriver                        0x0000000102600e4a chromedriver + 392778\n3   chromedriver                        0x000000010264c41d chromedriver + 701469\n4   chromedriver                        0x000000010264c5b1 chromedriver + 701873\n5   chromedriver                        0x0000000102690214 chromedriver + 979476\n6   chromedriver                        0x000000010266e89d chromedriver + 841885\n7   chromedriver                        0x000000010268d6d9 chromedriver + 968409\n8   chromedriver                        0x000000010266e613 chromedriver + 841235\n9   chromedriver                        0x000000010263f3da chromedriver + 648154\n10  chromedriver                        0x000000010263fd1e chromedriver + 650526\n11  chromedriver                        0x00000001029ca890 chromedriver + 4364432\n12  chromedriver                        0x00000001029cfc41 chromedriver + 4385857\n13  chromedriver                        0x00000001029afb2e chromedriver + 4254510\n14  chromedriver                        0x00000001029d0969 chromedriver + 4389225\n15  chromedriver                        0x00000001029a1e69 chromedriver + 4197993\n16  chromedriver                        0x00000001029f1b78 chromedriver + 4524920\n17  chromedriver                        0x00000001029f1d57 chromedriver + 4525399\n18  chromedriver                        0x0000000102a02783 chromedriver + 4593539\n19  libsystem_pthread.dylib             0x00007ff8061141d3 _pthread_start + 125\n20  libsystem_pthread.dylib             0x00007ff80610fbd3 thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "def get_data(query):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://www.forbes.com/search/?q={query}\")\n",
    "\n",
    "\n",
    "    # temps pour que e boutton \"More articles\" soit cliquable\n",
    "    more_articles_button = WebDriverWait(driver, 30).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, 'search-more'))\n",
    "    )\n",
    "\n",
    "    # Cliquer sur le bouton \"More articles\" jusqu'à ce qu'il n'y en ait plus\n",
    "    while more_articles_button:\n",
    "        try:\n",
    "            more_articles_button.click()\n",
    "            more_articles_button = WebDriverWait(driver, 30).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, 'search-more'))\n",
    "            )\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # récupérez le contenu dès que tous les articles sont chargés, \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    all_elt = soup.find_all(\"article\", class_=\"stream-item et-promoblock-removeable-item et-promoblock-star-item\")\n",
    "    titles = []\n",
    "    dates = []\n",
    "    url_articles = []\n",
    "    contents = []\n",
    "\n",
    "    # Récupération des contenus de chaque élément\n",
    "    for elt in all_elt:\n",
    "        title = elt.find(\"a\", class_=\"stream-item__title\").text\n",
    "\n",
    "        #vérifier si l'élément a été trouvé\n",
    "        url_article_elt = elt.find(\"a\", class_=\"stream-item__image ratio16x9\")\n",
    "        \n",
    "        if url_article_elt:\n",
    "            url_article = url_article_elt[\"href\"]\n",
    "        else : \n",
    "            url_article = \"Nan\"\n",
    "            \n",
    "        content = elt.find(\"div\", class_=\"stream-item__description\").text\n",
    "        date = elt.find(\"div\", class_=\"stream-item__date\").text\n",
    "\n",
    "        # Ajout des contenus à la liste\n",
    "        titles.append(title)\n",
    "        dates.append(date)\n",
    "        contents.append(content)\n",
    "        url_articles.append(url_article)\n",
    "\n",
    "    # Création du DataFrame\n",
    "    data = pd.DataFrame({\"Title\": titles, \"Date\": dates, \"Urls\": url_articles, \"Content\": contents})\n",
    "    \n",
    "    # Fermer le navigateur après avoir terminé\n",
    "    driver.quit()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Appeler la fonction avec l'URL openAI\n",
    "query = \"Finance\"\n",
    "df = get_data(query)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4fec4",
   "metadata": {},
   "source": [
    "#### TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b61a5f",
   "metadata": {},
   "source": [
    "First you need to create a Sentiment Analysis model with the IMDB database using Scikit-learn (or XGBoost).\n",
    "\n",
    "Dataset :  imdb.csv \n",
    "Using python, create a Kafka “consumer” with your trained-model to classify each articles from Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2742462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_with_encoding(file_path, encodings=['utf-8', 'latin-1', 'ISO-8859-1']):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "                decoded_content = content.decode(encoding, errors='replace')\n",
    "            return pd.read_csv(StringIO(decoded_content))\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise UnicodeDecodeError(f\"Unable to decode the file {file_path} with the provided encodings.\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "file_path = \"./imdb.csv\"\n",
    "data = read_csv_with_encoding(file_path)\n",
    "\n",
    "\n",
    "# Supposons que df est votre DataFrame\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Appliquer LabelEncoder aux colonnes d'objets\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "df = df.dropna(subset=['label'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_categorization(data):\n",
    "    \"\"\"\n",
    "    Cette fonction prend la donnée d'origine (data) et renvoie une version catégorisée de data.\n",
    "    Les classes sont représentées par des entiers.\n",
    "    \"\"\"\n",
    "    new_data = data.copy()\n",
    "    class_mapping = {\"neg\": 0, \"pos\": 1, \"unsup\": 2}\n",
    "    new_data['label'] = new_data['label'].map(class_mapping)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bade2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data(data, label_column=\"label\", feature_columns=[\"type\", \"review\", \"file\"]):\n",
    "    \"\"\"\n",
    "    Cette fonction prend la donnée d'origine (data) et crée une version catégorisée de cette donnée.\n",
    "    Elle renvoie les données fractionnées (données d'entraînement et données de test).\n",
    "    \"\"\"\n",
    "    new_data = data_categorization(data)\n",
    "    X = new_data[feature_columns]\n",
    "    y = new_data[label_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = get_split_data(data_imdb)\n",
    "X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Cette fonction prend X_train et y_train et renvoie un modèle XGBoost entraîné sur ces derniers.\n",
    "    \"\"\"\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_set = get_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157635a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standardization(X_train, X_test, model_type=\"XGBoost\"):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en paramètres X_train et X_test et retourne X_train et X_test standardisés.\n",
    "    Pour XGBoost, elle renvoie simplement les données telles quelles, car XGBoost n'exige généralement pas la standardisation.\n",
    "    \"\"\"\n",
    "    if model_type == \"XGBoost\":\n",
    "        return X_train, X_test\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled,X_test_scaled = data_standardization(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985551be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xgboost_importance(X_train, y_train, X_test, y_test, params=None, num_boost_round=100):\n",
    "    \"\"\"\n",
    "    Cette fonction prend X_train, y_train, X_test et y_test comme données d'entraînement et de test,\n",
    "    et affiche l'importance des fonctionnalités à l'aide de XGBoost ainsi que les indicateurs de performance.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame, features d'entraînement.\n",
    "    - y_train: Series, labels d'entraînement.\n",
    "    - X_test: DataFrame, features de test.\n",
    "    - y_test: Series, labels de test.\n",
    "    - params: dict, paramètres du modèle XGBoost (par défaut, utilisera des paramètres par défaut).\n",
    "    - num_boost_round: int, nombre d'itérations d'entraînement du modèle XGBoost.\n",
    "\n",
    "    Returns:\n",
    "    - None (affiche le graphique d'importance des fonctionnalités et les indicateurs de performance).\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        # Utilisez des paramètres par défaut si aucun paramètre n'est fourni\n",
    "        params = {'objective': 'multi:softmax', 'num_class': len(set(y_train)), 'eval_metric': 'mlogloss'}\n",
    "\n",
    "    # Créer un objet DMatrix pour les données XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "    # Entraîner le modèle XGBoost\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "    # Afficher l'importance des fonctionnalités\n",
    "    plot_importance(model)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculer et afficher les indicateurs de performance\n",
    "    y_pred = model.predict(xgb.DMatrix(X_test))\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # Afficher les indicateurs de performance\n",
    "    print('\\nPerformance Indicators')\n",
    "    print(\"==========================================\")\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'Confusion Matrix:\\n{cm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xgboost_importance(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1b829",
   "metadata": {},
   "source": [
    "#### LOAD\n",
    "\n",
    "Make another Kafka consumer to export articles and their label inside a PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38332124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e16c706",
   "metadata": {},
   "source": [
    "#### REAL-TIME DASHBOARD\n",
    "\n",
    "Create one PowerBI dashboard connected on the Kafka in order to monitor in real-time the number of\n",
    "articles coming by topics\n",
    "Create another PowerBI conencted to the PostgreSQL database to monitor the results of your classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
